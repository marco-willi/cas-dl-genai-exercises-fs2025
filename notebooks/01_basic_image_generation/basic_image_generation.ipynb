{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "try:\n",
    "    import jupyter_black\n",
    "\n",
    "    jupyter_black.load()\n",
    "except:\n",
    "    print(\"black not installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "title",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Basic Image Generation\n",
    "\n",
    "## Goals\n",
    "\n",
    "- Train and understand different basic generative models for image generation\n",
    "- Train a Generative Adversarial Network (GAN)\n",
    "- Train a Variational Autoencoder (VAE) and understand its latent space.\n",
    "- Train a Diffusion Model (DM) using `diffusers` components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a9856bbf98ea570f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Setup\n",
    "\n",
    "Let's define paths, install & load the necessary Python packages.\n",
    "\n",
    "**Optionally: Save the notebook to your personal google drive to persist changes.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mount your google drive to store data and results (if running the code in Google Colab)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "print(f\"In colab: {IN_COLAB}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modify the following paths if necessary.**\n",
    "\n",
    "That is where your data will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "if IN_COLAB:\n",
    "    DATA_PATH = Path(\"/content/drive/MyDrive/cas-dl-module-genai-part2\")\n",
    "else:\n",
    "    DATA_PATH = Path(\"../../data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install `dl_genai_lectures`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import dl_genai_lectures\n",
    "\n",
    "    print(\"dl_genau_lectures installed, all good\")\n",
    "except ImportError as e:\n",
    "    import os\n",
    "\n",
    "    if Path(\"/workspace/code/src\").exists():\n",
    "        print(\"Installing from local repo\")\n",
    "        os.system(\"cd /workspace/code  && pip install .\")\n",
    "    else:\n",
    "        print(\"Installing from git repo\")\n",
    "        os.system(\"pip install git+https://github.com/marco-willi/cas-dl-genai-exercises-fs2025\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from typing import Callable\n",
    "\n",
    "import lightning as L\n",
    "import numpy as np\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.v2 import functional as TF\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from dl_genai_lectures import visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a default device for your computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Prepare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the famous MNIST dataset because of its small size and illustrative power. Feel free to user a different dataset (which would need some adapations later for the models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "\n",
    "ds_mnist_train = MNIST(root=DATA_PATH.joinpath(\"mnist\"), train=True, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We inspect the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_mnist_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each element consists of a [PIL.Image](https://pillow.readthedocs.io/en/stable/reference/Image.html), a commonly used class to store and process images in Python, and a label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [ds_mnist_train[i][0] for i in range(0, 16)]\n",
    "labels = [f\"Label: {ds_mnist_train[i][1]}\" for i in range(0, 16)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix, ax = visualize.plot_collage(images, captions=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we select specific numbers to inspect their variations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_to_show = 16\n",
    "digit_to_select = 1\n",
    "\n",
    "ones = [i for i in range(len(ds_mnist_train)) if ds_mnist_train[i][1] == digit_to_select]\n",
    "\n",
    "ones = ones[0:num_to_show]\n",
    "images = [ds_mnist_train[i][0] for i in ones]\n",
    "labels = [f\"Label: {ds_mnist_train[i][1]}\" for i in ones]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix, ax = visualize.plot_collage(images, captions=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Inspect a few more digits and observe how they vary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a [lightning data module](https://lightning.ai/docs/pytorch/stable/data/datamodule.html) to handle our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataModule(L.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: str,\n",
    "        batch_size: int = 32,\n",
    "        num_workers: int = 2,\n",
    "        transform_fn: Callable | None = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "        if transform_fn is not None:\n",
    "            self.transform = transform_fn\n",
    "        else:\n",
    "            self.transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    def prepare_data(self):\n",
    "        MNIST(self.data_dir, train=True, download=True)\n",
    "        MNIST(self.data_dir, train=False, download=True)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            self.ds_train = MNIST(self.data_dir, train=True, transform=self.transform)\n",
    "\n",
    "        if stage == \"test\" or stage is None:\n",
    "            self.ds_test = MNIST(self.data_dir, train=False, transform=self.transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.ds_train,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.ds_test, batch_size=self.batch_size, num_workers=self.num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the data module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = MNISTDataModule(data_dir=DATA_PATH.joinpath(\"mnist\"))\n",
    "data_module.setup(\"fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = data_module.ds_train[0]\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = data_module.train_dataloader()\n",
    "image_batch, label_batch = next(iter(dl))\n",
    "image_batch.shape, label_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Generative Adversarial Networks\n",
    "\n",
    "In the following we will implement a Generative Adversarial Network (GAN) to generate samples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by implementing a Generator network, denoted by $\\mathcal{G}$, which takes as input a latent vector $\\mathbf{z} \\in \\mathbb{R}^d$, where the dimensionality $d$ is a configurable hyperparameter.\n",
    "\n",
    "Given that the task involves image generation, $\\mathcal{G}$ is implemented as a convolutional neural network (CNN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim=100):\n",
    "        super(Generator, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            # Map the latent vector to a flat feature map (6 channels of 7x7)\n",
    "            nn.Linear(self.latent_dim, 6 * 49),\n",
    "            # Reshape the flat vector into a 3D tensor (channels, height, width)\n",
    "            nn.Unflatten(1, (6, 7, 7)),\n",
    "            # First transposed convolution: upsample from 7x7 to 14x14\n",
    "            nn.ConvTranspose2d(6, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.LeakyReLU(),  # Activation for non-linearity\n",
    "            nn.BatchNorm2d(32),  # Normalize activations to stabilize training\n",
    "            # Second transposed convolution: upsample from 14x14 to 28x28\n",
    "            nn.ConvTranspose2d(32, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            # Final convolution to reduce channels from 32 to 1 (grayscale output)\n",
    "            nn.Conv2d(32, 1, kernel_size=1, stride=1, padding=0),\n",
    "            # TanH activation to ensure output pixel values are in [-1, 1]\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        # Forward pass through the generator network\n",
    "        img = self.model(z)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify the model's input and output shapes are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "generator = Generator(latent_dim=5)\n",
    "summary(generator, input_size=(32, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we implement a Discriminator network, denoted by $\\mathcal{D}$, which takes an image $\\mathbf{x}$ as input and outputs a single logit representing the unnormalized probability that the image is real (from the dataset) rather than generated (from $\\mathcal{G}(\\mathbf{z}))$.\n",
    "\n",
    "Since the input consists of images, a convolutional neural network (CNN) is an appropriate architecture for $\\mathcal{D}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            # First convolution: downsample the input from 28x28 to 14x14\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(),  # Non-linear activation\n",
    "            # nn.BatchNorm2d(32),  # Normalize activations to stabilize training\n",
    "            # Second convolution: downsample from 14x14 to 7x7\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            # nn.BatchNorm2d(64),\n",
    "            # Flatten the 3D feature map into a 1D vector\n",
    "            nn.Flatten(),\n",
    "            # Fully connected layer to project features into a lower-dimensional representation\n",
    "            nn.Linear(64 * 7 * 7, 1),\n",
    "            # nn.LeakyReLU(),\n",
    "            # # Final linear layer outputs a single logit (real vs. fake score)\n",
    "            # nn.Linear(64, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        # Forward pass through the discriminator network\n",
    "        logits = self.model(img)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build the whole GAN model. We use a [L.LightningModule](https://lightning.ai/docs/pytorch/stable/common/lightning_module.html) to help with managing the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(L.LightningModule):\n",
    "    def __init__(self, latent_dim=20):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.generator = Generator(latent_dim=latent_dim)\n",
    "        self.discriminator = Discriminator()\n",
    "        self.latent_dim = latent_dim\n",
    "        # manual control over optimization\n",
    "        self.automatic_optimization = False\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.generator(z)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x_real, _ = batch\n",
    "        optimizer_g, optimizer_d = self.optimizers()\n",
    "\n",
    "        # sample latent vectors\n",
    "        z = torch.randn((x_real.shape[0], self.latent_dim), device=self.device)\n",
    "        # z = z.type_as(x_real)\n",
    "\n",
    "        # ==== Train the Generator =====\n",
    "        self.toggle_optimizer(optimizer_g)\n",
    "        x_fake = self.generator(z)\n",
    "        logits_fake = self.discriminator(x_fake)\n",
    "        y_true = torch.ones_like(logits_fake)\n",
    "        # y_true = y_true.type_as(x_real)\n",
    "        g_loss = F.binary_cross_entropy_with_logits(logits_fake, y_true)\n",
    "        self.log(\"g_loss\", g_loss, prog_bar=True)\n",
    "        self.manual_backward(g_loss)\n",
    "        optimizer_g.step()\n",
    "        optimizer_g.zero_grad()\n",
    "        self.untoggle_optimizer(optimizer_g)\n",
    "\n",
    "        # ==== Train the Discriminator =====\n",
    "        self.toggle_optimizer(optimizer_d)\n",
    "\n",
    "        # Real images (label = 1 + noise for better convergence)\n",
    "        logits_real = self.discriminator(x_real)\n",
    "        y_real = 1.0 - torch.rand_like(logits_real) * 0.05\n",
    "        d_real_loss = F.binary_cross_entropy_with_logits(logits_real, y_real)\n",
    "\n",
    "        # Fake images (label = 0 + noise)\n",
    "        logits_fake = self.discriminator(x_fake.detach())\n",
    "        y_fake = torch.rand_like(logits_fake) * 0.05\n",
    "        d_fake_loss = F.binary_cross_entropy_with_logits(logits_fake, y_fake)\n",
    "\n",
    "        # discriminator loss is the average of these\n",
    "        d_loss = 0.5 * (d_real_loss + d_fake_loss)\n",
    "        self.log(\"d_loss\", d_loss, prog_bar=True)\n",
    "        self.manual_backward(d_loss)\n",
    "        optimizer_d.step()\n",
    "        optimizer_d.zero_grad()\n",
    "        self.untoggle_optimizer(optimizer_d)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer_generator = torch.optim.Adam(self.generator.parameters(), lr=1e-3)\n",
    "        optimizer_discriminator = torch.optim.Adam(self.discriminator.parameters(), lr=1e-3)\n",
    "        return optimizer_generator, optimizer_discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What loss value to you expect in the limit? (if both models are equally well in the minimax game)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess the quality of generative models it is important to monitor sample quality. A simple way to do this is by visual inspection. We implement a callback which regularily saves samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleMonitor(L.Callback):\n",
    "    def __init__(self, latent_dim, num_samples=16):\n",
    "        super().__init__()\n",
    "        self.num_samples = num_samples\n",
    "        self.test_z = torch.randn(16, latent_dim)\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        test_images = pl_module.forward(self.test_z.to(pl_module.device))\n",
    "        grid = make_grid(test_images)\n",
    "        pl_module.logger.experiment.add_image(\n",
    "            \"train/generated_images\", grid, trainer.current_epoch\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start a tensorboard server to observe training progress. \n",
    "\n",
    "You can open your browser at localhost:6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir={DATA_PATH.joinpath(\"lightning_logs\")} --host 0.0.0.0 --port=6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "logger = TensorBoardLogger(DATA_PATH.joinpath(\"lightning_logs\"), name=\"gan/\")\n",
    "\n",
    "L.seed_everything(123)\n",
    "\n",
    "data_module = MNISTDataModule(\n",
    "    data_dir=DATA_PATH.joinpath(\"mnist\"),\n",
    "    transform_fn=transforms.Compose([transforms.ToTensor(), transforms.Normalize(0.5, 0.5)]),\n",
    ")\n",
    "\n",
    "LATENT_DIM = 20\n",
    "\n",
    "gan_model = GAN(latent_dim=LATENT_DIM)\n",
    "trainer = L.Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    max_epochs=15,\n",
    "    logger=logger,\n",
    "    default_root_dir=DATA_PATH.joinpath(\"lightning_logs\"),\n",
    "    callbacks=[SampleMonitor(latent_dim=LATENT_DIM, num_samples=16)],\n",
    ")\n",
    "trainer.fit(gan_model, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Feel free to change the architecture in order to improve the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Sample from the model and display the result. You can look at the `SampleMonitor` class for inspiration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Variational Autoencoders\n",
    "\n",
    "In the following we will implement a Variational Autoencoder (VAE) to generate samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A VAE consists of an Encoder $\\mathcal{E}$ which takes as input images $\\mathbf{x}$ and outputs the parameters of a latent distribution $\\mathbf{z}$. \n",
    "\n",
    "We define $\\mathbf{z}$ to be normally distributed with a diagonal covariance matrix.\n",
    "\n",
    "The dimensionality of $\\mathbf{z}$ should be configurable.\n",
    "\n",
    "Since we are dealing with images, we use a convolutional neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(nn.Module):\n",
    "    \"\"\"Sample from the latent space using the mean (z_mean) and log variance (z_log_var)\"\"\"\n",
    "\n",
    "    def forward(self, z_mean, z_log_var):\n",
    "        std = torch.exp(0.5 * z_log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return z_mean + eps * std\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(7 * 7 * 64, 16),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.z_mean = nn.Linear(16, latent_dim)\n",
    "        self.z_log_var = nn.Linear(16, latent_dim)\n",
    "        self.sampling = Sampling()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # [N, C, H, W] -> [N, 16]\n",
    "        pre_z = self.network(x)\n",
    "\n",
    "        # get the mean and log variance of the latent space\n",
    "        # for the data point\n",
    "\n",
    "        # [N, 16] -> [N, latent_dim]\n",
    "        z_mean = self.z_mean(pre_z)\n",
    "        # [N, 16] -> [N, latent_dim]\n",
    "        z_log_var = self.z_log_var(pre_z)\n",
    "\n",
    "        # sample from the latent space using the mean and log variance\n",
    "\n",
    "        # [N, latent_dim], [N, latent_dim] -> [N, latent_dim]\n",
    "        z = self.sampling(z_mean, z_log_var)\n",
    "\n",
    "        return z_mean, z_log_var, z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if the encoder layers are correctly parameterized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "encoder = Encoder(latent_dim=2)\n",
    "\n",
    "summary(encoder, input_size=(16, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(nn.Linear(latent_dim, 7 * 7 * 64), nn.ReLU())\n",
    "\n",
    "        self.deconv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 1, kernel_size=3, padding=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.fc(z)\n",
    "        x = x.view(-1, 64, 7, 7)\n",
    "        x = self.deconv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "decoder = Decoder(latent_dim=2)\n",
    "\n",
    "summary(decoder, input_size=(16, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(L.LightningModule):\n",
    "    def __init__(self, latent_dim=20):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.encoder = Encoder(latent_dim=latent_dim)\n",
    "        self.decoder = Decoder(latent_dim=latent_dim)\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "        z_mean, z_log_var, z = self.encoder(x)\n",
    "        x_rec = self.decoder(z)\n",
    "        rec_loss = F.binary_cross_entropy(x_rec, x, reduction=\"none\").sum(dim=(1, 2, 3)).mean()\n",
    "        kl_loss = -0.5 * torch.sum(1 + z_log_var - z_mean**2 - torch.exp(z_log_var))\n",
    "        kl_loss = kl_loss / x.shape[0]\n",
    "        loss = rec_loss + kl_loss\n",
    "        self.log(\"rec_loss\", rec_loss, prog_bar=True)\n",
    "        self.log(\"kl_loss\", kl_loss, prog_bar=True)\n",
    "        self.log(\"loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess the quality of the generated samples we monitor sample quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAESampleMonitor(L.Callback):\n",
    "    def __init__(self, latent_dim, num_samples=16):\n",
    "        super().__init__()\n",
    "        self.num_samples = num_samples\n",
    "        self.test_z = torch.randn(16, latent_dim)\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        test_images = pl_module.forward(self.test_z.to(pl_module.device))\n",
    "        grid = make_grid(test_images)\n",
    "        pl_module.logger.experiment.add_image(\n",
    "            \"train/generated_images\", grid, trainer.current_epoch\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally we can also monitor the reconstruction quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAEReconstructionQualityMonitor(L.Callback):\n",
    "    def __init__(self, image_batch):\n",
    "        super().__init__()\n",
    "        self.image_batch = image_batch\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "\n",
    "        z_mean, z_log_var, z = pl_module.encoder(self.image_batch.to(pl_module.device))\n",
    "        image_batch_reconstructed = pl_module.decoder(z_mean).detach().cpu()\n",
    "\n",
    "        all_images = torch.cat([self.image_batch, image_batch_reconstructed])\n",
    "        grid = make_grid(all_images, nrow=self.image_batch.shape[0])\n",
    "        pl_module.logger.experiment.add_image(\n",
    "            \"train/reconstructed_images\", grid, trainer.current_epoch\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start the tensorboard server, if not already running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir={DATA_PATH.joinpath(\"lightning_logs\")} --host 0.0.0.0 --port=6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = TensorBoardLogger(DATA_PATH.joinpath(\"lightning_logs\"), name=\"vae/\")\n",
    "\n",
    "L.seed_everything(123)\n",
    "\n",
    "LATENT_DIM = 2\n",
    "MAX_EPOCHS = 15\n",
    "\n",
    "data_module = MNISTDataModule(data_dir=DATA_PATH.joinpath(\"mnist\"))\n",
    "data_module.setup(\"fit\")\n",
    "\n",
    "vae_model = VAE(latent_dim=LATENT_DIM)\n",
    "\n",
    "reconstruction_test_batch = torch.stack([data_module.ds_train[i][0] for i in range(16)], axis=0)\n",
    "callbacks_list = [\n",
    "    VAESampleMonitor(latent_dim=LATENT_DIM, num_samples=16),\n",
    "    VAEReconstructionQualityMonitor(image_batch=reconstruction_test_batch),\n",
    "]\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    logger=logger,\n",
    "    default_root_dir=DATA_PATH.joinpath(\"lightning_logs\"),\n",
    "    callbacks=callbacks_list,\n",
    ")\n",
    "trainer.fit(vae_model, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By training a VAE to map samples from a probabilistic latent space to images, we can now investigate the latent space itself.\n",
    "\n",
    "This works for visualizing 2 dimensional latent spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latent_space_samples(\n",
    "    generator: nn.Module,\n",
    "    num_points_per_dim=30,\n",
    "    latent_range=1.0,\n",
    "    figsize=15,\n",
    "):\n",
    "    from torchvision.transforms.v2 import functional as TF\n",
    "\n",
    "    # define latent space grid\n",
    "    latent_grid_x = np.linspace(-latent_range, latent_range, num_points_per_dim)\n",
    "    latent_grid_y = np.linspace(-latent_range, latent_range, num_points_per_dim)[::-1]\n",
    "\n",
    "    # sample from grid\n",
    "    generator.eval()\n",
    "    samples = list()\n",
    "    with torch.no_grad():\n",
    "        for x in latent_grid_x:\n",
    "            for y in latent_grid_y:\n",
    "                z_sample = torch.tensor([[x, y]], dtype=torch.float32).to(generator.device)\n",
    "                x_decoded = TF.to_pil_image(generator(z_sample).cpu().squeeze(0))\n",
    "                samples.append(x_decoded)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decode points in the latent space by sampling along a regular 2-D grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = get_latent_space_samples(vae_model, num_points_per_dim=30, latent_range=3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we display the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = visualize.plot_collage(samples, axes_iteration_order=\"C\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Explain the observed structure of the latent space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also exploit the fact that we have labels to inspect the latent space. We can simply encode original data points to the latent space and then color the points according to their class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot label clusters\n",
    "def plot_label_clusters(vae, dl, num_smples=1000):\n",
    "    vae.eval()\n",
    "    latents = []\n",
    "    labels = []\n",
    "    num_samples_processed = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in dl:\n",
    "            labels += y.numpy().tolist()\n",
    "            x = x.to(vae.device)\n",
    "            z_mean, _, _ = vae.encoder(x)\n",
    "            latents.append(z_mean.cpu().numpy())\n",
    "            num_samples_processed += x.shape[0]\n",
    "            if num_samples_processed >= num_smples:\n",
    "                break\n",
    "\n",
    "    z_mean = np.concatenate(latents, axis=0)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    sns.scatterplot(\n",
    "        x=z_mean[:, 0],\n",
    "        y=z_mean[:, 1],\n",
    "        hue=labels,\n",
    "        palette=\"tab10\",\n",
    "        s=12,\n",
    "        ax=ax,\n",
    "    )\n",
    "    ax.set_xlabel(\"z[0]\")\n",
    "    ax.set_ylabel(\"z[1]\")\n",
    "    ax.set_title(\"Latent space clusters\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 20000\n",
    "images = [ds_mnist_train[i][0] for i in range(0, NUM_SAMPLES)]\n",
    "labels = [ds_mnist_train[i][1] for i in range(0, NUM_SAMPLES)]\n",
    "\n",
    "data_module = MNISTDataModule(data_dir=DATA_PATH.joinpath(\"mnist\"))\n",
    "data_module.setup(\"fit\")\n",
    "dl = data_module.train_dataloader()\n",
    "\n",
    "\n",
    "plot_label_clusters(vae_model, dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Without using any labels in the training process, the latent space separates the different digits quite nicely. How could this be useful for other tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Try to increase sample quality. What happens with a larger latent space?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Diffusion Models\n",
    "\n",
    "Diffusion models (DMs) consist of two processes:\n",
    "\n",
    "- a _forward|noise|diffusion process_ which gradually adds random Gaussian noise to the input $\\mathbf{x}$. It can also be considered an encoder.\n",
    "- a _reverse|denoising process_ which learns how to (gradually) remove noise as added by the _forward process_. It can also be considered a decoder.\n",
    "\n",
    "DMs are latent variable models where the latent variable $\\mathbf{z}$ has the same dimensionality as the input data $\\mathbf{x}$.\n",
    "\n",
    "We follow the original implementation [Denoising Diffusion Probabilistic Models](http://arxiv.org/abs/2006.11239). The notation of the variables follows the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by implementing the forward process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardProcess(nn.Module):\n",
    "    def __init__(self, T=1000, beta_0=1e-4, beta_T=0.02):\n",
    "        super().__init__()\n",
    "        self.beta_T = beta_T\n",
    "        self.beta_0 = beta_0\n",
    "        self.T = T\n",
    "\n",
    "        alpha_bars, alphas, betas = self._precompute_variances()\n",
    "        self.register_buffer(\"alpha_bar_ts\", torch.tensor(alpha_bars))\n",
    "        self.register_buffer(\"alpha_ts\", 1 - torch.tensor(alphas))\n",
    "        self.register_buffer(\"beta_ts\", 1 - torch.tensor(betas))\n",
    "\n",
    "    def get_beta(self, t):\n",
    "        return (((self.beta_T - self.beta_0) / self.T) * t) + self.beta_0\n",
    "\n",
    "    def _precompute_variances(self):\n",
    "        betas = list()\n",
    "        beta = self.get_beta(1)\n",
    "        betas.append(beta)\n",
    "\n",
    "        alpha = list()\n",
    "        alpha.append(1 - beta)\n",
    "\n",
    "        alpha_bar = list()\n",
    "        alpha_bar.append(1 - self.get_beta(1))\n",
    "\n",
    "        for t in range(2, self.T + 1):\n",
    "            beta = self.get_beta(t)\n",
    "            alpha_t = 1 - beta\n",
    "            alpha_bar.append(alpha_bar[-1] * alpha_t)\n",
    "            alpha.append(alpha_t)\n",
    "            betas.append(beta)\n",
    "        return alpha_bar, alpha, betas\n",
    "\n",
    "    def forward(self, x_0, t):\n",
    "\n",
    "        N, C, H, W = x_0.shape\n",
    "\n",
    "        device = x_0.device\n",
    "\n",
    "        eps = torch.randn_like(x_0).to(device)\n",
    "\n",
    "        alpha_bar = self.alpha_bar_ts[t].view(N, 1, 1, 1).to(device)\n",
    "\n",
    "        mean = torch.sqrt(alpha_bar) * x_0\n",
    "        std = torch.sqrt(1.0 - alpha_bar)\n",
    "        x_t = mean + std * eps\n",
    "\n",
    "        return x_t, eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fw = ForwardProcess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we test the forward process. We use $T=1000$ for the number of steps where we add noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 1000\n",
    "betas = np.array([fw.get_beta(t) for t in range(1, T + 1)])\n",
    "_ = sns.lineplot(betas).set(xlabel=\"Timestep [t]\", ylabel=r\"$\\beta_t$\", title=\"Noise Schedule\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above shows how the noise variance increases over time. Here a linear schedule is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = fw.alpha_bar_ts.numpy()\n",
    "_ = sns.lineplot(alphas).set(\n",
    "    xlabel=\"Timestep [t]\", ylabel=r\"$\\bar{\\alpha}_t$\", title=\"Signal to Noise Ratio\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above shows how the ratio between signal and noise. We see that, in the limit, no signal is left and the the result of the diffusion process is identical to that of a Gaussian distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://github.com/pytorch/vision/blob/main/gallery/assets/dog2.jpg?raw=true\"\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "image = Image.open(io.BytesIO(r.content))\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert the image to a tensor and scale it into [-1, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.v2.functional as TF\n",
    "\n",
    "x = TF.to_image(image).to(torch.float32)\n",
    "\n",
    "from torchvision.transforms import Compose, Normalize, ToPILImage, ToTensor\n",
    "\n",
    "transf = Compose([ToTensor(), Normalize(0.5, 0.5)])\n",
    "inverse_transf = Compose([Normalize(0.0, 1.0 / 0.5), Normalize(-0.5, 1.0), ToPILImage()])\n",
    "\n",
    "\n",
    "x = transf(image)\n",
    "x_batch = x.unsqueeze(0)\n",
    "x_rec = inverse_transf(x)\n",
    "\n",
    "np.testing.assert_allclose(\n",
    "    np.array(list(image.getdata())), np.array(list(x_rec.getdata())), atol=1\n",
    ")\n",
    "x.shape\n",
    "x.max()\n",
    "x.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, noise_added = fw(x_batch, 0)\n",
    "output.shape\n",
    "image_rec = inverse_transf(output.squeeze(0))\n",
    "image_rec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets take a look at the image degradation over multiple steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = np.linspace(0, 1000 - 1, 10).astype(int)\n",
    "steps\n",
    "\n",
    "noised_images = list()\n",
    "for t in steps:\n",
    "    noised_images.append(inverse_transf(fw(x_batch, t)[0].squeeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = visualize.plot_collage(\n",
    "    noised_images, captions=[f\"t={s + 1}\" for s in steps], nrows=1, ncols=len(steps)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We simplify things going forward and use a forward process from the diffusers library. This also includes tricks like  clipping the latent representations to a specified range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDPMScheduler\n",
    "\n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly test it,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.randn_like(x_batch)\n",
    "steps = np.linspace(0, 1000 - 1, 10).astype(int)\n",
    "\n",
    "noised_images = list()\n",
    "for t in steps:\n",
    "    output = noise_scheduler.add_noise(\n",
    "        x_batch, noise=noise, timesteps=torch.tensor([t], device=x.device)\n",
    "    )\n",
    "    noised_images.append(inverse_transf(output.squeeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = visualize.plot_collage(\n",
    "    noised_images, captions=[f\"t={s + 1}\" for s in steps], nrows=1, ncols=len(steps)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks very similar!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forward process seems to work. Now we need to implement the backward or _Denoising_ process.\n",
    "\n",
    "This is a neural network that estimates the noise added to an image at a given time step.\n",
    "\n",
    "We use a Unet with convolutional layers. More details can be found here: [Keras Example](https://keras.io/examples/generative/ddpm/)\n",
    "\n",
    "We use the HuggingFace Library to configure our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import UNet2DModel\n",
    "\n",
    "rp = UNet2DModel(\n",
    "    sample_size=32,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    layers_per_block=2,\n",
    "    block_out_channels=(32, 32, 64, 128),\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"UpBlock2D\",\n",
    "        \"AttnUpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(rp, sample=torch.randn((16, 1, 32, 32)), timestep=1)\n",
    "\n",
    "x_out = rp(sample=torch.randn((1, 1, 32, 32)).to(rp.device), timestep=0)\n",
    "x_out[\"sample\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build our model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DM(L.LightningModule):\n",
    "    def __init__(self, T=1000, beta_0=1e-4, beta_T=0.02):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.noise_scheduler = DDPMScheduler(\n",
    "            num_train_timesteps=T,\n",
    "            beta_start=beta_0,\n",
    "            beta_end=beta_T,\n",
    "        )\n",
    "        self.denoising = UNet2DModel(\n",
    "            sample_size=32,\n",
    "            in_channels=1,\n",
    "            out_channels=1,\n",
    "            layers_per_block=2,\n",
    "            block_out_channels=(32, 32, 64, 128),\n",
    "            down_block_types=(\n",
    "                \"DownBlock2D\",\n",
    "                \"DownBlock2D\",\n",
    "                \"AttnDownBlock2D\",\n",
    "                \"DownBlock2D\",\n",
    "            ),\n",
    "            up_block_types=(\n",
    "                \"UpBlock2D\",\n",
    "                \"AttnUpBlock2D\",\n",
    "                \"UpBlock2D\",\n",
    "                \"UpBlock2D\",\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x_T):\n",
    "\n",
    "        N, C, H, W = x_T.shape\n",
    "        device = x_T.device\n",
    "        x_t = x_T\n",
    "\n",
    "        for t in reversed(range(0, self.noise.T)):\n",
    "            t_tensor = torch.full((N,), t, device=device, dtype=torch.int)\n",
    "\n",
    "            # Predict noise\n",
    "            eps_hat = self.denoising(x_t, t_tensor)[\"sample\"]\n",
    "\n",
    "            beta_t = self.noise.beta_ts[t].view(N, 1, 1, 1)\n",
    "            alpha_t = self.noise.alpha_ts[t].view(N, 1, 1, 1)\n",
    "            alpha_bar_t = self.noise.alpha_bar_ts[t].view(N, 1, 1, 1)\n",
    "\n",
    "            # Compute the mean of the reverse process\n",
    "            mean = (1 / torch.sqrt(alpha_t)) * (\n",
    "                x_t - ((1 - alpha_t) / torch.sqrt(1 - alpha_bar_t)) * eps_hat\n",
    "            )\n",
    "\n",
    "            # Add noise if t > 1\n",
    "            if t > 0:\n",
    "                noise = torch.randn_like(x_t)\n",
    "                sigma_t = torch.sqrt(beta_t)\n",
    "                x_t = mean + sigma_t * noise\n",
    "            else:\n",
    "                x_t = mean  # final denoised sample\n",
    "\n",
    "        return x_t\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x_zero, _ = batch\n",
    "\n",
    "        batch_size = x_zero.shape[0]\n",
    "\n",
    "        # sample timestemps\n",
    "        timesteps = torch.randint(\n",
    "            0,\n",
    "            self.noise_scheduler.config.num_train_timesteps,\n",
    "            (batch_size,),\n",
    "            device=x_zero.device,\n",
    "        )\n",
    "\n",
    "        # noise input\n",
    "        eps = torch.randn(x_zero.shape).to(x_zero.device)\n",
    "        x_t = self.noise_scheduler.add_noise(x_zero, eps, timesteps)\n",
    "\n",
    "        # estimate noise\n",
    "        noise_estimate = self.denoising(x_t, timesteps)[\"sample\"]\n",
    "\n",
    "        # compute loss\n",
    "        loss = F.mse_loss(noise_estimate, eps)\n",
    "\n",
    "        self.log(\"loss\", loss, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monitor the quality of the generated samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDPMPipeline\n",
    "\n",
    "\n",
    "class DMSampleMonitor(L.Callback):\n",
    "    def __init__(self, latent_dim, num_samples=16):\n",
    "        super().__init__()\n",
    "        self.num_samples = num_samples\n",
    "        self.test_z = torch.randn((num_samples,) + latent_dim)\n",
    "\n",
    "    def on_fit_start(self, trainer, pl_module):\n",
    "        self.pipe = DDPMPipeline(unet=pl_module.denoising, scheduler=pl_module.noise_scheduler)\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        test_images = self.pipe(batch_size=self.num_samples, output_type=\"pil\").images\n",
    "        test_images = [TF.to_tensor(image) for image in test_images]\n",
    "        grid = make_grid(test_images)\n",
    "        pl_module.logger.experiment.add_image(\n",
    "            \"train/generated_images\", grid, trainer.current_epoch\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's monitor our progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir={DATA_PATH.joinpath(\"lightning_logs\")} --host 0.0.0.0 --port=6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = TensorBoardLogger(DATA_PATH.joinpath(\"lightning_logs\"), name=\"dm/\")\n",
    "\n",
    "L.seed_everything(123)\n",
    "\n",
    "LATENT_DIM = (1, 32, 32)\n",
    "MAX_EPOCHS = 15\n",
    "NUM_STEPS = 1000\n",
    "\n",
    "transf = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),  # Converts to [0, 1] float tensor\n",
    "        transforms.Normalize(0.5, 0.5),  # Scale to [-1, 1]\n",
    "        transforms.Pad(padding=2, fill=0),  # Zero-pad 2 pixels on all sides\n",
    "    ]\n",
    ")\n",
    "\n",
    "inverse_transf = transforms.Compose(\n",
    "    [\n",
    "        transforms.CenterCrop((28, 28)),\n",
    "        transforms.Normalize(0.0, 1.0 / 0.5),\n",
    "        transforms.Normalize(-0.5, 1.0),\n",
    "        transforms.ToPILImage(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "data_module = MNISTDataModule(data_dir=DATA_PATH.joinpath(\"mnist\"), transform_fn=transf)\n",
    "\n",
    "data_module.setup(\"fit\")\n",
    "\n",
    "dm_model = DM(T=NUM_STEPS, beta_0=1e-4, beta_T=0.02)\n",
    "\n",
    "\n",
    "callbacks_list = [\n",
    "    DMSampleMonitor(latent_dim=LATENT_DIM, num_samples=16),\n",
    "]\n",
    "\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    logger=logger,\n",
    "    default_root_dir=DATA_PATH.joinpath(\"lightning_logs\"),\n",
    "    callbacks=callbacks_list,\n",
    ")\n",
    "trainer.fit(dm_model, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can sample from the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDPMPipeline\n",
    "\n",
    "pipe = DDPMPipeline(unet=dm_model.denoising, scheduler=dm_model.noise_scheduler).to(device)\n",
    "images = pipe(output_type=\"pil\", batch_size=32).images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = visualize.plot_collage(images, nrows=6, ncols=6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
