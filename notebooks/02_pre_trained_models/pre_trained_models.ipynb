{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "try:\n",
    "    import jupyter_black\n",
    "\n",
    "    jupyter_black.load()\n",
    "except:\n",
    "    print(\"black not installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "title",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Pre-Trained Models and APIs\n",
    "\n",
    "## Goals\n",
    "\n",
    "- Learn how to use pre-trained models on HuggingFace\n",
    "- Learn how to condition the generation process\n",
    "- Learn how to use APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a9856bbf98ea570f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Setup\n",
    "\n",
    "Let's define paths, install & load the necessary Python packages.\n",
    "\n",
    "**Optionally: Save the notebook to your personal google drive to persist changes.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mount your google drive to store data and results (if running the code in Google Colab)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "print(f\"In colab: {IN_COLAB}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modify the following paths if necessary.**\n",
    "\n",
    "That is where your data will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "if IN_COLAB:\n",
    "    DATA_PATH = Path(\"/content/drive/MyDrive/cas-dl-module-genai-part2\")\n",
    "else:\n",
    "    DATA_PATH = Path(\"../../data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install `dl_genai_lectures`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import dl_genai_lectures\n",
    "\n",
    "    print(\"dl_genau_lectures installed, all good\")\n",
    "except ImportError as e:\n",
    "    import os\n",
    "\n",
    "    if Path(\"/workspace/code/src\").exists():\n",
    "        print(\"Installing from local repo\")\n",
    "        os.system(\"cd /workspace/code  && pip install .\")\n",
    "    else:\n",
    "        print(\"Installing from git repo\")\n",
    "        os.system(\"pip install git+https://github.com/marco-willi/cas-dl-genai-exercises-fs2025\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "\n",
    "import diffusers\n",
    "import numpy as np\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision.transforms.v2 import functional as TF\n",
    "\n",
    "from dl_genai_lectures import visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a default device for your computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Text-to-Image Generation\n",
    "\n",
    "\n",
    "Stable diffusion is a well known open-source model for image generation.\n",
    "\n",
    "First we need to log into hugging face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we initialize a pipeline (and also download all model assets if not already available):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id, cache_dir=DATA_PATH.joinpath(\"HF_CACHE\"))\n",
    "pipe = pipe.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can generate images!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A futuristic city floating in the clouds, artstation style\"\n",
    "generator = torch.Generator(device=device).manual_seed(123)\n",
    "image = pipe(prompt, generator=generator).images[0]\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different parameters that guide the diffusion process, such as \"guidance_scale\". Let's see how the generation changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator(device=device).manual_seed(123)\n",
    "image = pipe(prompt, guidance_scale=1.0, generator=generator).images[0]\n",
    "generator = torch.Generator(device=device).manual_seed(123)\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator(device=device).manual_seed(123)\n",
    "image = pipe(prompt, guidance_scale=9.0, generator=generator).images[0]\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Which do you like better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Conditional Image Generation\n",
    "\n",
    "Image generation can be controlled by different condtioning factors, IF a model was trained with those factors.\n",
    "\n",
    "A well-known conditioning is canny edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "url = \"https://github.com/pytorch/vision/blob/main/gallery/assets/dog2.jpg?raw=true\"\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "image = Image.open(io.BytesIO(r.content))\n",
    "\n",
    "image = np.array(image)\n",
    "\n",
    "low_threshold = 100\n",
    "high_threshold = 200\n",
    "\n",
    "image = cv2.Canny(image, low_threshold, high_threshold)\n",
    "image = image[:, :, None]\n",
    "image = np.concatenate([image, image, image], axis=2)\n",
    "canny_image = Image.fromarray(image)\n",
    "canny_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import ControlNetModel, StableDiffusionControlNetPipeline\n",
    "from diffusers.utils import load_image\n",
    "from torchvision import transforms\n",
    "\n",
    "# Load the ControlNet model conditioned on Canny edges\n",
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    \"lllyasviel/sd-controlnet-canny\",\n",
    "    # torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    controlnet=controlnet,\n",
    "    safety_checker=None,\n",
    "    # torch_dtype=torch.float16\n",
    ").to(device)\n",
    "\n",
    "pipe.enable_xformers_memory_efficient_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the edge map is a 3-channel PIL image\n",
    "canny_image_condition = canny_image.convert(\"RGB\")\n",
    "\n",
    "prompt = (\n",
    "    \"A dog lying on the grass, photorealistic, 8k resolution, highly detailed, cinematic lighting\"\n",
    ")\n",
    "generator = torch.Generator(device=device).manual_seed(123)\n",
    "\n",
    "output = pipe(prompt, image=canny_image_condition, num_inference_steps=30, generator=generator)\n",
    "output.images[0].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Generation on low ressource hardware\n",
    "\n",
    "Modern (image) generative models require a lot of ressources, particularly, VRAM. There are several options to reduce ressources, some depend on additional libraries:\n",
    "\n",
    "- reduce model precision from floating point (32 bit) to half-precision (16 bit) or even much further to (8-bit or 4-bit)  [bitsandbytes](https://github.com/bitsandbytes-foundation/bitsandbytes)\n",
    "- off-load computations to cpu (RAM)\n",
    "- use efficient implementations of operations, e.g [xformers](https://github.com/facebookresearch/xformers)\n",
    "\n",
    "Here is the link to a HuggingFace guide:  [Reduce memory usage](https://huggingface.co/docs/diffusers/en/optimization/memory)\n",
    "\n",
    "\n",
    "Note: some options are hardware dependent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "model_id = \"dreamlike-art/dreamlike-photoreal-2.0\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "pipe = pipe.to(device)\n",
    "\n",
    "pipe.enable_vae_slicing()\n",
    "pipe.enable_xformers_memory_efficient_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens\"\n",
    "image = pipe(prompt).images[0]\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task** Try some of the tricks when running the following model. Warning: This is a large model that requires a huge download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import (\n",
    "    BitsAndBytesConfig,\n",
    "    SD3Transformer2DModel,\n",
    "    StableDiffusion3Pipeline,\n",
    ")\n",
    "\n",
    "model_id = \"stabilityai/stable-diffusion-3.5-medium\"\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "model_nf4 = SD3Transformer2DModel.from_pretrained(\n",
    "    model_id,\n",
    "    subfolder=\"transformer\",\n",
    "    quantization_config=nf4_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "pipeline = StableDiffusion3Pipeline.from_pretrained(\n",
    "    model_id, transformer=model_nf4, torch_dtype=torch.bfloat16\n",
    ")\n",
    "pipeline.enable_model_cpu_offload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A whimsical and creative image depicting a hybrid creature that is a mix of a waffle and a hippopotamus, basking in a river of melted butter amidst a breakfast-themed landscape. It features the distinctive, bulky body shape of a hippo. However, instead of the usual grey skin, the creature's body resembles a golden-brown, crispy waffle fresh off the griddle. The skin is textured with the familiar grid pattern of a waffle, each square filled with a glistening sheen of syrup. The environment combines the natural habitat of a hippo with elements of a breakfast table setting, a river of warm, melted butter, with oversized utensils or plates peeking out from the lush, pancake-like foliage in the background, a towering pepper mill standing in for a tree.  As the sun rises in this fantastical world, it casts a warm, buttery glow over the scene. The creature, content in its butter river, lets out a yawn. Nearby, a flock of birds take flight\"\n",
    "\n",
    "image = pipeline(\n",
    "    prompt=prompt,\n",
    "    num_inference_steps=40,\n",
    "    guidance_scale=4.5,\n",
    "    max_sequence_length=512,\n",
    ").images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Using APIs - Example \"Replicate\"\n",
    "\n",
    "Explore the API and the models: https://replicate.com/explore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import replicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pil_to_replicate_file(pil_image, format=\"PNG\"):\n",
    "    \"\"\"\n",
    "    Converts a PIL image to a file-like object compatible with Replicate API.\n",
    "    Args:\n",
    "        pil_image: PIL.Image object\n",
    "        format: 'PNG' or 'JPEG'\n",
    "    Returns:\n",
    "        io.BytesIO object with image data\n",
    "    \"\"\"\n",
    "    byte_stream = io.BytesIO()\n",
    "    pil_image.save(byte_stream, format=format)\n",
    "    byte_stream.seek(0)  # Reset cursor to the beginning\n",
    "    return byte_stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLICATE_API_TOKEN = os.getenv(\"REPLICATE_API_TOKEN\")\n",
    "\n",
    "if REPLICATE_API_TOKEN is None or REPLICATE_API_TOKEN == \"\":\n",
    "    REPLICATE_API_TOKEN = getpass(\"Enter your Replicate API key: \")\n",
    "    os.environ[\"REPLICATE_API_TOKEN\"] = REPLICATE_API_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text to Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = replicate.run(\n",
    "    \"black-forest-labs/flux-schnell\",\n",
    "    input={\n",
    "        \"prompt\": \"A futuristic city floating in the clouds, artstation style\",\n",
    "    },\n",
    ")\n",
    "\n",
    "image_data = output[0].read()\n",
    "image = Image.open(io.BytesIO(image_data))\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face Swap Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://replicate.delivery/pbxt/Mb44Wp0W7Xfa1Pp91zcxDzSSQQz8GusUmXQXi3GGzRxDvoCI/0_1.webp\"\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "swap_image = Image.open(io.BytesIO(r.content)).convert(\"RGB\")\n",
    "swap_image\n",
    "\n",
    "\n",
    "url = \"https://replicate.delivery/pbxt/Mb44XIUHkUrmyyH1OP5K1WmFN7SNN0eUSU16A8rBtuXe7eYV/cyberpunk_80s_example.png\"\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "target_image = Image.open(io.BytesIO(r.content)).convert(\"RGB\")\n",
    "target_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = replicate.run(\n",
    "    \"easel/advanced-face-swap\",\n",
    "    input={\n",
    "        \"swap_image\": pil_to_replicate_file(swap_image),\n",
    "        \"hair_source\": \"target\",\n",
    "        \"user_gender\": \"default\",\n",
    "        \"target_image\": pil_to_replicate_file(target_image),\n",
    "        \"user_b_gender\": \"default\",\n",
    "    },\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data = output.read()\n",
    "image = Image.open(io.BytesIO(image_data))\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Upscaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://m.media-amazon.com/images/M/MV5BMTQxNzI3MjE2OF5BMl5BanBnXkFtZTcwNjk4MjAzMw@@._V1_QL75_UX603_.jpg\"\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "lowres_image = Image.open(io.BytesIO(r.content)).convert(\"RGB\")\n",
    "lowres_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = replicate.run(\n",
    "    \"recraft-ai/recraft-crisp-upscale\",\n",
    "    input={\n",
    "        \"image\": pil_to_replicate_file(lowres_image),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data = output.read()\n",
    "image = Image.open(io.BytesIO(image_data))\n",
    "image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
